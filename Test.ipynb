{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbc3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.masutils.ml2 import ML2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a1f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Theory:\n",
      "            CRISP-DM (Cross Industry Standard Process for Data Mining)\n",
      "                Phases:\n",
      "                    ● Business Understanding\n",
      "                    ● Data Understanding\n",
      "                    ● Data Preparation\n",
      "                    ● Modeling\n",
      "                    ● Evaluation\n",
      "                    ● Deployment\n",
      "\n",
      "            Assumptions of Logistic Regression:\n",
      "                ● Independence of error, whereby all sample group outcomes are separate from each other \n",
      "                    (i.e., there are no duplicate responses)\n",
      "                ● Linearity in the logit for any continuous independent variables\n",
      "                ● Absence of multicollinearity\n",
      "                ● Lack of strongly influential outliers\n",
      "\n",
      "            Logistic Regression Evaluation Metrics:\n",
      "                ● Deviance\n",
      "                ● AIC\n",
      "                ● Pseudo R2:\n",
      "                    ● McFadden R2\n",
      "                    ● Cox-Snell R2\n",
      "                    ● Nagelkerke R2\n",
      "\n",
      "            Logistic Regression Perfromance Measures:\n",
      "                ● Confusion matrix:\n",
      "                    ● Accuracy : (TP+TN)/(TP+TN+FP+FN)\n",
      "                    ● Precision : (TP)/(TP+FP)\n",
      "                    ● Recall : (TP)/(TP+FN)\n",
      "                    ● False Positive Rate : (FP)/(FP+TN) (1-Specificity)\n",
      "                    ● Specificity : (TN)/(TN+FP)\n",
      "                    ● F1 score : 2*(Precision*Recall)/(Precision+Recall)\n",
      "                    ● Kappa\n",
      "                ● Cross Entropy\n",
      "                ● ROC: (TPR/FPR)\n",
      "                ● Youden's Index: max(TPR - FPR)\n",
      "\n",
      "            How to Handle Imbalanced Data:\n",
      "                ● Up sample minority class (SMOTE)\n",
      "                ● Down sample majority class\n",
      "                ● Change the performance metric\n",
      "                ● Try synthetic sampling approach\n",
      "                ● Use different algorithm\n",
      "\n",
      "            Naive Bayes:\n",
      "                Assumptions of Naive Bayes Classification:\n",
      "                    ● The predictors are independent of each other\n",
      "                    ● All the predictors have an equal effect on the outcome.\n",
      "                \n",
      "                Naive Bayes Procedure:\n",
      "                    ● Obtain Frequency of Predictors\n",
      "                    ● Compute Likelihood of predictores and obtain prior probs based on train data\n",
      "                    ● Compute Posterior Probabilities for each class labels\n",
      "                    ● Assign most probable class\n",
      "\n",
      "                Naive Bayes Advantages:\n",
      "                    ● Easy to implement in the case of text analytics problems\n",
      "                    ● Used for multiple class prediction problems\n",
      "                    ● Performs better for categorical data than numeric data\n",
      "                Naive Bayes Disadvantages:\n",
      "                    ● Fails to find relationship among features\n",
      "                    ● May not perform when the data has more number of predictor \n",
      "                    ● The assumption of independence among features may not always hold good\n",
      "\n",
      "            KNN:\n",
      "                Proximity/Similarity Measures:\n",
      "                    ● Manhattan (L1)\n",
      "                    ● Euclidean (L2)\n",
      "                    ● Minkowski (LN)\n",
      "                    ● Chebychev (L-inf)\n",
      "\n",
      "                KNN Procedure:\n",
      "                    ● Choose a Distance measure and K\n",
      "                    ● Compute Distance between point whose label is to be identified and other points\n",
      "                    ● Sort distances in asc order\n",
      "                    ● Choose K data points having shortest distances and note the corresponding labels.\n",
      "                    ● Label which has highest freq is assigned to the point.\n",
      "\n",
      "                KNN Advantages:\n",
      "                    ● Easy to implement\n",
      "                    ● No training required\n",
      "                    ● New data can be added at any time\n",
      "                    ● Effective if training data is large\n",
      "                KNN Disadvantages:\n",
      "                    ● To chose apt value for K\n",
      "                    ● Computational expensive\n",
      "                    ● Can not tell which features gives the best result\n",
      "\n",
      "                KNN Applications:\n",
      "                    ● Image classification\n",
      "                    ● Handwriting recognition\n",
      "                    ● Predict credit rating of customers\n",
      "                    ● Replace missing values\n",
      "\n",
      "\n",
      "            Information Theory:\n",
      "\n",
      "                Entropy:\n",
      "                    ● Shannon's Entropy:\n",
      "                        ● Entropy is the measure of information for classification problem, \n",
      "                            i.e. it measures the heterogeneity of a feature\n",
      "                        ● The entropy of a feature is calculated = - sigma(1 to c) [pc log2 pc]\n",
      "                            where pc is prob of occurence of class.\n",
      "                    ● Conditional Entropy:\n",
      "                        ● The conditional entropy of one feature given other is calculated as from the \n",
      "                            contingency table of the two features.\n",
      "                        ● E(T|X) = sigma(x belongs to X) [P(c)*E(c)]\n",
      "                Information Gain:\n",
      "                    ● Information gain is the decrease in entropy at a node:\n",
      "                    ● IG(T,X) = E(T) - E(T|X)\n",
      "                        where T and X are events..\n",
      "                    ● IG CANNOT BE NEGATIVE.\n",
      "\n",
      "                Entropy for Numeric Features:\n",
      "                    ● Sort the data in ascending order and compute the midpoints of the successive values. \n",
      "                        These midpoints act as the thresholds\n",
      "                    ● For each of these threshold values, enumerate through all possible values \n",
      "                        to compute the information gain\n",
      "                    ● Select the value which has the highest information gain\n",
      "\n",
      "                \n",
      "            Decision Trees:\n",
      "\n",
      "                Terminologies/Parts:\n",
      "                    ● Branch/Subtree: (a subsection of the entire decision tree.)\n",
      "                    ● Root Node: (no incoming edge and zero or more outgoing edges.)\n",
      "                    ● Internal Node: (exactly one incoming edge and zero or more outgoing edges.)\n",
      "                    ● Leaf/Terminal Node: (exactly one incoming edge and no outgoing edge)\n",
      "                    \n",
      "                Measures of Purity of a Node:\n",
      "                    ● Entropy: Shannon/Conditional\n",
      "                    ● Gini Index: 1 - sigma(c=1,n) [pc**2]\n",
      "                        IG using Gini Index:\n",
      "                            IG(T|X) = GiniIndex(T) - GiniIndex(T|X)\n",
      "                    ● Classification Error:\n",
      "                        1 - max(pc**2)\n",
      "\n",
      "                Consruction of Decision Trees:\n",
      "\n",
      "                Decision Tree Algorithms:\n",
      "                    ID3:\n",
      "                        ● Handles only categorical data\n",
      "                        ● May not converge to optimal decision tree\n",
      "                        ● May overfit\n",
      "                    C4.5:\n",
      "                        ● Extension to ID3 algorithm\n",
      "                        ● Handles both categorical and numeric data\n",
      "                        ● Handles the missing data marked by '?'\n",
      "                    C5.0:\n",
      "                        ● Works faster than C4.5 algorithm\n",
      "                        ● More memory efficient than C4.5 algorithm\n",
      "                        ● Creates smaller trees with similar efficiency\n",
      "\n",
      "                Overfitting in Decision Trees:\n",
      "                    Overfitted Trees:\n",
      "                        ● may have leaf nodes that contain only one sample, ie. singleton node\n",
      "                        ● are complicated and long decision chains\n",
      "                    Handing using Pruning:\n",
      "                        ● Pruning is a technique that removes the branches of a tree that provide little \n",
      "                            power to classify instances, thus reduces the size of the tree\n",
      "                        ● Pruning reduces the complexity of the tree\n",
      "                        ● Pruning can be achieved in two ways:\n",
      "                            ○ Pre-Pruning: The decision tree stops growing before the tree completely grown\n",
      "                            ○ Post-Pruning: The decision tree is allowed to grow completely and then prune\n",
      "                        \n",
      "            Ensemble Learning:\n",
      "                ● Bagging:\n",
      "                    Homogenous Models can be built independently and outputs are aggregated at end.\n",
      "                    Ex: Random Forest\n",
      "                ● Boosting:\n",
      "                    Homogenous Models can be built sequentially\n",
      "                    Previous Model influences features of successive models.\n",
      "                    Ex: AdaBoost\n",
      "                ● Stacking:\n",
      "                    Outputs from several base models are the inputs for Meta Model.\n",
      "\n",
      "            Random Forest Classfier:\n",
      "                ● Procedure:\n",
      "                    ● Take Sample Set of M samples, N features\n",
      "                    ● Bootstrap Sampling\n",
      "                    ● Make different Training sets\n",
      "                    ● Get all the different Predictions\n",
      "                    ● Aggregate all the predictions.\n",
      "\n",
      "            Feature Importance:\n",
      "                ● Gini Importance:\n",
      "                    ● Also known as mean decrease impurity\n",
      "                    ● It is the average total decrease in the node impurity weighted by the probability of \n",
      "                        reaching it.\n",
      "                    ● The average is taken over all the trees of the random forest\n",
      "                ● Mean Decrease in accuracy:\n",
      "                    ● Measure the decrease in accuracy on the out-of-bag data\n",
      "                    ● Basically, the idea is to measure the decrease in accuracy on OOB data when you randomly \n",
      "                        permute the values for that feature.\n",
      "                    ● If the decrease is low, then the feature is not important, and vice-versa.\n",
      "\n",
      "            Bagging (Bootstrap Aggregation):\n",
      "\n",
      "                ● Designed to improve the stability (small change in dataset change the model) and accuracy of \n",
      "                    classification and regression models\n",
      "                ● It reduces variance errors and helps to avoid overfitting\n",
      "                ● Can be used with any type of machine learning model, mostly used with Decision Tree\n",
      "                ● Uses sampling with replacement to generate multiple samples of a given size. Sample may \n",
      "                    contain repeat data points\n",
      "                ● For large sample size, sample data is expected to have roughly 63.2% ( 1 - 1/e) unique data \n",
      "                    points and the rest being duplicates\n",
      "                ● For classification bagging is used with voting to decide the class of an input while for \n",
      "                    regression average or median values are calculate\n",
      "\n",
      "\t\t\tBoosting Techniques:\n",
      "\t\t\t\tAdvantages of Boosting:\n",
      "\t\t\t\t\t● Enhances the efficiency of weak classifiers \n",
      "\t\t\t\t\t● Both precision and recall can be enhanced through boosting algorithms\n",
      "\t\t\t\tDisadvantages of Boosting:\n",
      "\t\t\t\t\t● Loss of simplicity and explanatory power\n",
      "\t\t\t\t\t● Increased computational complexity\n",
      "\n",
      "\t\t\tBoosting vs Bagging:\n",
      "\t\t\t\tBAGGING:\n",
      "\t\t\t\t\t● Base learners learn is parallel \n",
      "\t\t\t\t\t● Random sampling \n",
      "\t\t\t\t\t● Reduces variance \n",
      "\t\t\t\tBOOSTING:\n",
      "\t\t\t\t\t● Base learners learn sequentially\n",
      "\t\t\t\t\t● Non-random sampling\n",
      "\t\t\t\t\t● Reduces bias and variance \n",
      "\n",
      "\t\t\tAdaboost Procedure:\n",
      "\t\t\t\t● Assign weights to each of the sample\n",
      "\t\t\t\t● Build stumps with each variable. Calculate the Gini Index or Entropy for each variable \n",
      "\t\t\t\t● AdaBoost picks the variable with the smallest Gini Index for building the stump\n",
      "\t\t\t\t● Determine the 'Amount of Say' the stump will have in the final prediction. \n",
      "\t\t\t\t● Incorrectly classified records from the 1st stump have a greater chance of being \n",
      "                    passed to the next stump\n",
      "\t\t\t\t● AdaBoost does this by, increasing the weights of the wrongly classified samples and \n",
      "                    decreasing the weights of the correctly classified samples\n",
      "\t\t\t\t● Increase the sample weights of incorrectly classified samples, by the formula.\n",
      "\t\t\t\t● Decrease the sample weights of correctly classified samples, by the formula.\n",
      "\t\t\t\t● Normalise the weights so that all the new sample weights add up to 1. This can be done \n",
      "                    by dividing each of the new sample weight by the sum of new sample weights\n",
      "\t\t\t\t● Adaboost creates a new collection of samples based on the normalized updated weights\n",
      "\t\t\t\t● Start with an empty dataset. Randomly pick a number between 0 and 1\n",
      "\t\t\t\t● The algorithm will check this random number falls into which bucket and populate the \n",
      "                    corresponding record in the new dataset.\n",
      "\t\t\t\t● Add samples to the new dataset by choosing random numbers as explained in step 7, till \n",
      "                    the new dataset is the same size as the original dataset\n",
      "\t\t\t\t● Get rid of the original samples and use the new collection of samples, for the next \n",
      "                    stump\n",
      "\t\t\t\t● Go back to the beginning and find the variable to make the next stump that does the \n",
      "                    best job at classifying the new collection of sample.\n",
      "\t\t\t\t\n",
      "\t\t\tGradient Boosting:\n",
      "\t\t\t\t● Start with an initial leaf which is the initial prediction for all samples\n",
      "\t\t\t\t● Compute the Residuals\n",
      "\t\t\t\t● Build a tree for the residuals\n",
      "\t\t\t\t● Calculate the output( in terms of log(Odds)) for each leaf of the tree\n",
      "\t\t\t\t● Output of all the Leaf node is tabulated \n",
      "\t\t\t\t● Scale the output by learning rate\n",
      "\t\t\t\t● Update the prediction\n",
      "\t\t\t\t● Convert Log(Odds) to probability\n",
      "\t\t\t\t● Calculate the Residuals again\n",
      "\t\t\t\t● Build the second tree \n",
      "\t\t\t\t● GBM repeats these steps until it has built the specified number of trees or the \n",
      "                    residuals are very small or reach a threshold\n",
      "\n",
      "\t\t\tAdaboost vs GBM:\n",
      "\t\t\t\tSimilarities:\n",
      "\t\t\t\t\t● Decision trees are the base learners\n",
      "\t\t\t\t\t● Trees are built based on the errors made by the previous trees\n",
      "\t\t\t\t\t● The size of the tree is restricted\n",
      "\t\t\t\t\t● Trees are scaled \n",
      "\t\t\t\tDifferences:\n",
      "\t\t\t\t\tAdaBoost Gradient Boost\n",
      "\t\t\t\t\t\t● Mostly made of stumps (Tree with a root and two leaves)\n",
      "\t\t\t\t\t\t● Stumps are scaled such that each stump has different amount of say in the \n",
      "                            final prediction\n",
      "\t\t\t\t\t\t● Shortcoming of the base learner is identified by high-weight samples\n",
      "\t\t\t\t\tGBM:\n",
      "\t\t\t\t\t\t● Starts with a leaf. Then build trees with 8 to 32 leaves. So, it does \n",
      "                            restrict the size of the trees\n",
      "\t\t\t\t\t\t● Output from each tree is scaled by a learning rate, however all trees \n",
      "                            have an equal amount of say in the final prediction\n",
      "\t\t\t\t\t\t● Shortcoming of the base learner is identified by gradients\n",
      "\t\t\t\t\n",
      "\t\t\tXGBoost:\n",
      "\t\t\t\tFeatures:\n",
      "\t\t\t\t\t● Regularization\n",
      "\t\t\t\t\t● A unique decision tree\n",
      "\t\t\t\t\t● Approximate Greedy algorithm\n",
      "\t\t\t\t\t● Weighted Quantile Sketch\n",
      "\t\t\t\t\t● Sparsity-Aware split finding\n",
      "\t\t\t\t\t● Parallel learning\n",
      "\t\t\t\t\t● Cache-Aware Access\n",
      "\t\t\t\t\t● Blocks for Out of Core Computations\n",
      "\n",
      "\t\t\tVoting:\n",
      "\t\t\t\tThis is used for classification problem statements and Multiple base model can be \n",
      "                    created using using the same dataset with different algorithms\n",
      "                    ● Majority voting:\n",
      "\t\t\t\t\t\tEvery model makes a prediction (votes) for each test instance and the final \n",
      "                        output prediction is the one that receives more than half of the votes.\n",
      "\t\t\t\t\t● Weighted voting:\n",
      "\t\t\t\t\t\tUnlike majority voting, where each model has the same rights, we can increase \n",
      "                        the importance of one or more models.\n",
      "\t\t\tStacking:\n",
      "\t\t\t\tThe basic idea is to train machine learning algorithms with training dataset and then \n",
      "                    generate a new dataset with these models.\n",
      "\t\t\t\tFeatures:\n",
      "\t\t\t\t\t● The combiner system should learn how the base systems make errors\n",
      "\t\t\t\t\t● Stacking is a means of estimating and rectifying the errors in the base learners\n",
      "\t\t\t\t\t● Therefore, the combiner should be trained on the data unused in training the base \n",
      "                        learners\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(ML2.st_theory.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc819234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.masutils.ml3 import ML3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9992d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        K-Means Clustering:\n",
      "\n",
      "            Plot WCSS (Elbow Plot):\n",
      "                def plot_wcss():\n",
      "                    wcss  = []\n",
      "                    for i in range(1,21):\n",
      "                        kmeans = KMeans(n_clusters = i, random_state = 10)\n",
      "                        kmeans.fit(X)\n",
      "                        wcss.append(kmeans.inertia_)\n",
      "                            pass\n",
      "\n",
      "                    plt.plot(range(1,21), wcss)\n",
      "                    plt.title('Elbow Plot', fontsize = 15)\n",
      "                    plt.xlabel('No. of clusters (K)', fontsize = 15)\n",
      "                    plt.ylabel('WCSS', fontsize = 15)\n",
      "                    plt.axvline(x = 5, color = 'red')                        \n",
      "                    plt.show()\n",
      "\n",
      "            K-Value using Silhouette Score:\n",
      "                n_clusters = [2, 3, 4, 5, 6]\n",
      "                for K in n_clusters:\n",
      "                    cluster = KMeans (n_clusters= K, random_state= 10)\n",
      "                    predict = cluster.fit_predict(X)\n",
      "                    score = silhouette_score(X, predict, random_state= 10)\n",
      "                    print (\"For {} clusters the silhouette score is {})\".format(K, score))\n",
      "\n",
      "            Silhouette Plot:\n",
      "                from yellowbrick.cluster import SilhouetteVisualizer\n",
      "                X=data_dime_input_merged_pca_applied\n",
      "                n_clusters=[2,3,4,5,6,7,8,9,10]\n",
      "                for K in n_clusters:\n",
      "                    model=KMeans(n_clusters=K,random_state=42)\n",
      "                    viz = SilhouetteVisualizer(model).fit(X)\n",
      "                    viz.show()\n",
      "\n",
      "            Dendrogram Plot:\n",
      "                from scipy.cluster.hierarchy import linkage,dendrogram\n",
      "                plt.figure(figsize=(12,10))\n",
      "                ward_merge=linkage(data_dime_input_merged,method='ward')\n",
      "                dendrogram(ward_merge,truncate_mode='lastp',p=50)\n",
      "                plt.show()\n",
      "\n",
      "            Build Clusters:\n",
      "                new_clusters = KMeans(n_clusters = 5, random_state = 10)\n",
      "                new_clusters.fit(X)\n",
      "                df_cust['Cluster'] = new_clusters.labels_\n",
      "\n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('K-means Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "\n",
      "        Hierarchical Clustering:\n",
      "\n",
      "            Build Linkage Matrix:\n",
      "                #instantiate linkage object with scaled data and consider 'ward' linkage method \n",
      "                link_mat = linkage(features_scaled, method = 'ward')     \n",
      "                #print first 10 observations of the linkage matrix 'link_mat'\n",
      "                print(link_mat[0:10])\n",
      "            \n",
      "            Plot Dendrogram:\n",
      "                from scipy.cluster.hierarchy import linkage,dendrogram\n",
      "                plt.figure(figsize=(12,10))\n",
      "                ward_merge=linkage(data_dime_input_merged,method='ward')\n",
      "                dendrogram(ward_merge,truncate_mode='lastp',p=50)\n",
      "                plt.show()\n",
      "\n",
      "            Calculate Cophenet Coefficient:\n",
      "                from sklearn.metrics.pairwise import euclidean_distances\n",
      "                eucli_dist = euclidean_distances(features_scaled)\n",
      "                dist_array = eucli_dist[np.triu_indices(5192, k = 1)]\n",
      "                coeff, cophenet_dist = cophenet(link_mat, dist_array)\n",
      "                print(coeff)\n",
      "\n",
      "            K-Value using Silhouette Score:\n",
      "                n_clusters = [2, 3, 4, 5, 6]\n",
      "                for K in n_clusters:\n",
      "                    cluster = AgglomerativeClustering (n_clusters= K, random_state= 10, linkage='ward')\n",
      "                    predict = cluster.fit_predict(X)\n",
      "                    score = silhouette_score(X, predict, random_state= 10)\n",
      "                    print (\"For {} clusters the silhouette score is {})\".format(K, score))\n",
      "\n",
      "            Build Clusters:\n",
      "                clusters = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
      "                clusters.fit(features_scaled)\n",
      "                df_prod['Cluster'] = clusters.labels_\n",
      "                df_prod['Cluster'].value_counts()\n",
      "\n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('Hierarchical Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "        \n",
      "        DBSCAN:\n",
      "            Build Model:\n",
      "                model = DBSCAN(eps = 0.8, min_samples = 15)\n",
      "                model.fit(features_scaled)\n",
      "                df_prod['Cluster_DBSCAN'] = model.labels_\n",
      "            \n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('DBSCAN Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "\n",
      "            **Interpretation**: \n",
      "                We can see that the algorithm has identified most of the technical products as the outliers.\n",
      "                Here we can see that the DBSCAN algorithm has not grouped the product like hierarchical clustering. Thus we can conclude that the DBSCAN algorithm is working poorly on this dataset.\n",
      "\n",
      "        PCA:\n",
      "\n",
      "            Normalize Data:\n",
      "                X = iris.data\n",
      "                X_std = StandardScaler().fit_transform(X)\n",
      "            Calculate Cov Matrix:\n",
      "                cov_matrix = np.cov(X_std.T)\n",
      "                print('Covariance Matrix \n",
      "%s', cov_matrix)\n",
      "            PairPlot:\n",
      "                sns.pairplot(X_std_df)\n",
      "                plt.tight_layout()\n",
      "            Calculate EigenVals and Eigne Vecs:\n",
      "                eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
      "            Cumulative Variance Explained:\n",
      "                tot = sum(eig_vals)\n",
      "                var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
      "                cum_var_exp = np.cumsum(var_exp)\n",
      "                print(\"Cumulative Variance Explained\", cum_var_exp)\n",
      "            \n",
      "        Cases:\n",
      "\n",
      "            DT Before Applying PCA:\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                from sklearn import tree\n",
      "                model=tree.DecisionTreeClassifier()\n",
      "                model.fit(X_train,y_train)\n",
      "                y_pred_DT = model.predict(X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT))\n",
      "                print(accuracy_score(y_test, y_pred_DT))\n",
      "\n",
      "            DT After applying PCA:\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                from sklearn.decomposition import PCA\n",
      "                pca = PCA()\n",
      "                X_train_2 = pca.fit_transform(X_train)\n",
      "                X_test_2 = pca.transform(X_test)\n",
      "                explained_variance = pca.explained_variance_ratio_\n",
      "                from sklearn import tree\n",
      "                model2=tree.DecisionTreeClassifier(random_state=1)\n",
      "                model2.fit(X_train_2,y_train)\n",
      "                y_pred_DT_2 = model2.predict(X_test_2)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT_2), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT_2))\n",
      "                print(accuracy_score(y_test, y_pred_DT_2))\n",
      "\n",
      "            DT After Applying LDA:\n",
      "                from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "                model = LinearDiscriminantAnalysis()\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                model.fit(X_train,y_train)\n",
      "                y_pred = model.predict(X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n",
      "                print(classification_report(y_test,y_pred))\n",
      "                print(accuracy_score(y_test, y_pred))\n",
      "\n",
      "                LDA Hyperparameter Tuning:\n",
      "                    from sklearn.model_selection import GridSearchCV\n",
      "                    from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "                    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "                    model = LinearDiscriminantAnalysis()\n",
      "                    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "                    grid = dict()\n",
      "                    grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
      "                    search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "                    results = search.fit(X_train, y_train)\n",
      "                    print('Mean Accuracy: %.3f' % results.best_score_)\n",
      "                    print('Config: %s' % results.best_params_)\n",
      "\n",
      "                    (With the Best Params, repeat the above LDA steps with Best Params)\n",
      "\n",
      "            DT After Applying Kernel PCA: ( Too Complex, Will not be asked! )\n",
      "\n",
      "            DT After Applying MCA:\n",
      "                ! pip install prince\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                cat = []\n",
      "                num = []\n",
      "                for i in X.columns:\n",
      "                    if X[i].nunique() < X.shape[0]/4:\n",
      "                        cat.append(i)\n",
      "                    else:\n",
      "                        num.append(i)\n",
      "                X_train_cat = X_train[cat]\n",
      "                X_train_num = X_train[num]\n",
      "                X_test_cat = X_test[cat]\n",
      "                X_test_num = X_test[num]\n",
      "                import prince\n",
      "                from sklearn.metrics import make_scorer\n",
      "                from sklearn.metrics import explained_variance_score\n",
      "                mca = prince.MCA()\n",
      "                mca_X_train = mca.fit_transform(X_train_cat)\n",
      "                mca_X_test = mca.fit_transform(X_test_cat)\n",
      "                main_mca_X_train = pd.concat([mca_X_train,X_train_num],axis = 1)\n",
      "                main_mca_X_test = pd.concat([mca_X_test,X_test_num],axis = 1)\n",
      "                explained_var = make_scorer(explained_variance_score)\n",
      "                explained_var\n",
      "                from sklearn import tree\n",
      "                model2=tree.DecisionTreeClassifier()\n",
      "                model2.fit(main_mca_X_train,y_train)\n",
      "                y_pred_DT_2 = model2.predict(main_mca_X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT_2), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT_2))\n",
      "                print(accuracy_score(y_test, y_pred_DT_2))\n",
      "\n",
      "\n",
      "\n",
      "        Recommendation Systems:\n",
      "\n",
      "            Popularity Based Recommendation:\n",
      "                x=df.groupby('ItemID').agg({'Rating':'mean','ItemID':'count'})\n",
      "                x.rename(columns={'ItemID':'CountofReviews'},inplace=True)\n",
      "                x[x['CountofReviews']>50].sort_values(by='Rating',ascending=False).head(10)\n",
      "\n",
      "            Content Based Recommendation:\n",
      "                Preprocessing:\n",
      "                    genres=data1['genres'].str.split('|',expand=True)\n",
      "                    genres=genres.fillna('Others')\n",
      "                    genres.columns=['genre1','genre2','genre3']\n",
      "                    data1=pd.concat([data1,genres],axis=1)\n",
      "                    data2=data1[movie_feat]\n",
      "                    data3=pd.get_dummies(data2)\n",
      "                    data3=data3.dropna()\n",
      "                \n",
      "                from sklearn.neighbors import NearestNeighbors\n",
      "                rec_model = NearestNeighbors(metric = 'cosine')\n",
      "                rec_model.fit(data3)\n",
      "                query_movie_index=200\n",
      "                dist, ind = rec_model.kneighbors(data3.iloc[query_movie_index, :].values.reshape(1, -1), n_neighbors = 6)\n",
      "                list(data3.index[ind[0]])[1:]\n",
      "                for i in range(0, len(dist[0])):\n",
      "                    if i == 0:\n",
      "                        print('Top 5 Recommendations for the user who watched the movie :',data3.index[query_movie_index])\n",
      "                    else:\n",
      "                        print(i, data3.index[ind[0][i]])\n",
      "\n",
      "\n",
      "            Collaborative Recommendation:\n",
      "                from surprise import Dataset,Reader\n",
      "                from surprise.model_selection import train_test_split,cross_validate\n",
      "                from surprise import KNNWithMeans,SVDpp\n",
      "                from surprise import accuracy\n",
      "                reader=Reader(rating_scale=(1,5))\n",
      "                rating_data=Dataset.load_from_df(data_recom[['UserID','ItemID','Rating']],reader)\n",
      "                [trainset,testset]=train_test_split(rating_data,test_size=0.15,shuffle=True)\n",
      "                trainsetfull=rating_data.build_full_trainset()\n",
      "                print(\"Number of users :\",trainsetfull.n_users,'\n",
      "')\n",
      "                print(\"Number of items :\",trainsetfull.n_items,'\n",
      "')\n",
      "                my_k = 15\n",
      "                my_min_k = 5\n",
      "                my_sim_option = {'name':'pearson', 'user_based':False}\n",
      "                algo = KNNWithMeans(k = my_k, min_k = my_min_k, sim_options = my_sim_option, verbose = True)\n",
      "                results = cross_validate(\n",
      "                    algo = algo, data = rating_data, measures=['RMSE'], \n",
      "                    cv=5, return_train_measures=True\n",
      "                    )\n",
      "                print(results['test_rmse'].mean())\n",
      "                algo.fit(trainsetfull)\n",
      "                algo.predict(uid = 50, iid =2)\n",
      "                len(ratings['movieId'].unique())\n",
      "                item_id_unique=ratings['movieId'].unique()\n",
      "                item_id10=ratings.loc[ratings['userId']==10,'movieId']\n",
      "                item_id_pred=np.setdiff1d(item_id_unique,item_id10)\n",
      "                testset=[[50,iid,5] for iid in item_id_pred]\n",
      "                pred=algo.test(testset)\n",
      "                pred\n",
      "                pred_ratings=np.array([pred1.est for pred1 in pred])\n",
      "                i_max=pred_ratings.argmax()\n",
      "                iid=item_id_pred[i_max]\n",
      "                print(\"Top item for user 10 has iid {0} with predicted rating {1}\".format(iid,pred_ratings[i_max]))\n",
      "\n",
      "                Compare Different Algorithms Scoreboard:\n",
      "\n",
      "                    from surprise import NormalPredictor\n",
      "                    from surprise import KNNBasic\n",
      "                    from surprise import KNNWithMeans\n",
      "                    from surprise import KNNWithZScore\n",
      "                    from surprise import KNNBaseline\n",
      "                    from surprise import SVD\n",
      "                    from surprise import BaselineOnly\n",
      "                    from surprise import SVDpp\n",
      "                    from surprise import NMF\n",
      "                    from surprise import SlopeOne\n",
      "                    from surprise import CoClustering\n",
      "                    benchmark = []\n",
      "                    # Iterate over all algorithms\n",
      "                    algorithms = [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]\n",
      "                    print (\"Attempting: \", str(algorithms), '\n",
      "\n",
      "\n",
      "')\n",
      "                    for algorithm in algorithms:\n",
      "                        print(\"Starting: \" ,str(algorithm))\n",
      "                        results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
      "                        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
      "                        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
      "                        benchmark.append(tmp)\n",
      "                        print(\"Done: \" ,str(algorithm), \"\n",
      "\n",
      "\")\n",
      "                    print ('\n",
      "\tDONE\n",
      "')\n",
      "                    surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
      "                    surprise_results\n",
      "\n",
      "            Apriori Algorithm:\n",
      "                from mlxtend.frequent_patterns import apriori\n",
      "                from mlxtend.frequent_patterns import association_rules\n",
      "\n",
      "                unique_row_items = []\n",
      "                for index, row in df.iterrows():\n",
      "                    items_series = str(row[0]).split(',')\n",
      "                    for item in items_series:\n",
      "                        if item not in unique_row_items:\n",
      "                            unique_row_items.append(item)\n",
      "                df_apriori = pd.DataFrame(columns=unique_row_items)\n",
      "                for index, row in df.iterrows():\n",
      "                    items = str(row[0]).split(',')\n",
      "                    one_hot_encoding = np.zeros(len(unique_row_items),dtype=int)\n",
      "                    for it in items:\n",
      "                        for i,column in enumerate(df_apriori.columns):\n",
      "                            #print(i,column,it)\n",
      "                            if it == column:\n",
      "                                one_hot_encoding[i] = 1\n",
      "                    df_apriori.at[index] = one_hot_encoding\n",
      "                df_apriori=df_apriori.astype('int')\n",
      "                freq_items = apriori(df_apriori, min_support = 0.2, use_colnames = True, verbose = 1)\n",
      "                df_association_rules = association_rules(freq_items, metric = \"confidence\", min_threshold = 0.2)\n",
      "                df_association_rules.sort_values(\"confidence\",ascending=False)\n",
      "                cols = ['antecedents','consequents']\n",
      "                df_association_rules[cols] = df_association_rules[cols].applymap(lambda x: tuple(x))#.apply(lambda x: str(x))\n",
      "                print (df_association_rules)\n",
      "                df_association_rules = (df_association_rules.explode('antecedents')\n",
      "                    .reset_index(drop=True)\n",
      "                    .explode('consequents')\n",
      "                    .reset_index(drop=True))\n",
      "                df_association_rules[\"product_group\"] = df_association_rules[\"antecedents\"].apply(lambda x: str(x)) + \",\" + df_association_rules[\"consequents\"].apply(lambda x: str(x))\n",
      "                df1 = df_association_rules.loc[:,[\"product_group\",\"confidence\",\"lift\"]].sort_values(\"confidence\",ascending=False)\n",
      "            \n",
      "                Plot:\n",
      "                    sns.barplot(x=\"product_group\",y=\"confidence\",data=df1)\n",
      "                    sns.barplot(x=\"product_group\",y=\"confidence\",hue=\"lift\",data=df1);\n",
      "                    df1.plot.bar()\n",
      "\n",
      "                Example Conclusion:\n",
      "                    * 80% of customers who buy MAGGI (Instant soup) buy it in tea.\n",
      "                    * TEA and MAGGI products increase their sales by 2.17 times mutually.\n",
      "                    * 66% of customers who buy SUGAR buy it in bread.\n",
      "                    * 42% of customers who buy COFFEE buy sugar and CORNFLAKES. At the same time, 33% of these sales are in bread.\n",
      "                \n",
      "            Hybrid Recommendation System: ( Too Complicated, wont be asked. )\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(ML3.st_2_model_build_steps.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d0a9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        K-Means Clustering:\n",
      "\n",
      "            Plot WCSS (Elbow Plot):\n",
      "                def plot_wcss():\n",
      "                    wcss  = []\n",
      "                    for i in range(1,21):\n",
      "                        kmeans = KMeans(n_clusters = i, random_state = 10)\n",
      "                        kmeans.fit(X)\n",
      "                        wcss.append(kmeans.inertia_)\n",
      "                            pass\n",
      "\n",
      "                    plt.plot(range(1,21), wcss)\n",
      "                    plt.title('Elbow Plot', fontsize = 15)\n",
      "                    plt.xlabel('No. of clusters (K)', fontsize = 15)\n",
      "                    plt.ylabel('WCSS', fontsize = 15)\n",
      "                    plt.axvline(x = 5, color = 'red')                        \n",
      "                    plt.show()\n",
      "\n",
      "            K-Value using Silhouette Score:\n",
      "                n_clusters = [2, 3, 4, 5, 6]\n",
      "                for K in n_clusters:\n",
      "                    cluster = KMeans (n_clusters= K, random_state= 10)\n",
      "                    predict = cluster.fit_predict(X)\n",
      "                    score = silhouette_score(X, predict, random_state= 10)\n",
      "                    print (\"For {} clusters the silhouette score is {})\".format(K, score))\n",
      "\n",
      "            Silhouette Plot:\n",
      "                from yellowbrick.cluster import SilhouetteVisualizer\n",
      "                X=data_dime_input_merged_pca_applied\n",
      "                n_clusters=[2,3,4,5,6,7,8,9,10]\n",
      "                for K in n_clusters:\n",
      "                    model=KMeans(n_clusters=K,random_state=42)\n",
      "                    viz = SilhouetteVisualizer(model).fit(X)\n",
      "                    viz.show()\n",
      "\n",
      "            Dendrogram Plot:\n",
      "                from scipy.cluster.hierarchy import linkage,dendrogram\n",
      "                plt.figure(figsize=(12,10))\n",
      "                ward_merge=linkage(data_dime_input_merged,method='ward')\n",
      "                dendrogram(ward_merge,truncate_mode='lastp',p=50)\n",
      "                plt.show()\n",
      "\n",
      "            Build Clusters:\n",
      "                new_clusters = KMeans(n_clusters = 5, random_state = 10)\n",
      "                new_clusters.fit(X)\n",
      "                df_cust['Cluster'] = new_clusters.labels_\n",
      "\n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('K-means Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "\n",
      "        Hierarchical Clustering:\n",
      "\n",
      "            Build Linkage Matrix:\n",
      "                #instantiate linkage object with scaled data and consider 'ward' linkage method \n",
      "                link_mat = linkage(features_scaled, method = 'ward')     \n",
      "                #print first 10 observations of the linkage matrix 'link_mat'\n",
      "                print(link_mat[0:10])\n",
      "            \n",
      "            Plot Dendrogram:\n",
      "                from scipy.cluster.hierarchy import linkage,dendrogram\n",
      "                plt.figure(figsize=(12,10))\n",
      "                ward_merge=linkage(data_dime_input_merged,method='ward')\n",
      "                dendrogram(ward_merge,truncate_mode='lastp',p=50)\n",
      "                plt.show()\n",
      "\n",
      "            Calculate Cophenet Coefficient:\n",
      "                from sklearn.metrics.pairwise import euclidean_distances\n",
      "                eucli_dist = euclidean_distances(features_scaled)\n",
      "                dist_array = eucli_dist[np.triu_indices(5192, k = 1)]\n",
      "                coeff, cophenet_dist = cophenet(link_mat, dist_array)\n",
      "                print(coeff)\n",
      "\n",
      "            K-Value using Silhouette Score:\n",
      "                n_clusters = [2, 3, 4, 5, 6]\n",
      "                for K in n_clusters:\n",
      "                    cluster = AgglomerativeClustering (n_clusters= K, random_state= 10, linkage='ward')\n",
      "                    predict = cluster.fit_predict(X)\n",
      "                    score = silhouette_score(X, predict, random_state= 10)\n",
      "                    print (\"For {} clusters the silhouette score is {})\".format(K, score))\n",
      "\n",
      "            Build Clusters:\n",
      "                clusters = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
      "                clusters.fit(features_scaled)\n",
      "                df_prod['Cluster'] = clusters.labels_\n",
      "                df_prod['Cluster'].value_counts()\n",
      "\n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('Hierarchical Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "        \n",
      "        DBSCAN:\n",
      "            Build Model:\n",
      "                model = DBSCAN(eps = 0.8, min_samples = 15)\n",
      "                model.fit(features_scaled)\n",
      "                df_prod['Cluster_DBSCAN'] = model.labels_\n",
      "            \n",
      "            Visualize All Cluster:\n",
      "                sns.scatterplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster')\n",
      "                plt.title('DBSCAN Clustering (for K=5)', fontsize = 15)\n",
      "                plt.xlabel('Spending Score', fontsize = 15)\n",
      "                plt.ylabel('Annual Income', fontsize = 15)\n",
      "                plt.show()\n",
      "\n",
      "            Cluster-Wise Analysis (do for each cluster):\n",
      "                def describe_cluster(cluster_id):\n",
      "                    print(len(df_cust[df_cust['Cluster'] == cluster_id]))\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe())\n",
      "                    print(df_cust[df_cust.Cluster==cluster_id].describe(include = object))\n",
      "\n",
      "            **Interpretation**: \n",
      "                We can see that the algorithm has identified most of the technical products as the outliers.\n",
      "                Here we can see that the DBSCAN algorithm has not grouped the product like hierarchical clustering. Thus we can conclude that the DBSCAN algorithm is working poorly on this dataset.\n",
      "\n",
      "        PCA:\n",
      "\n",
      "            Normalize Data:\n",
      "                X = iris.data\n",
      "                X_std = StandardScaler().fit_transform(X)\n",
      "            Calculate Cov Matrix:\n",
      "                cov_matrix = np.cov(X_std.T)\n",
      "                print('Covariance Matrix \n",
      "%s', cov_matrix)\n",
      "            PairPlot:\n",
      "                sns.pairplot(X_std_df)\n",
      "                plt.tight_layout()\n",
      "            Calculate EigenVals and Eigne Vecs:\n",
      "                eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
      "            Cumulative Variance Explained:\n",
      "                tot = sum(eig_vals)\n",
      "                var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
      "                cum_var_exp = np.cumsum(var_exp)\n",
      "                print(\"Cumulative Variance Explained\", cum_var_exp)\n",
      "            \n",
      "        Cases:\n",
      "\n",
      "            DT Before Applying PCA:\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                from sklearn import tree\n",
      "                model=tree.DecisionTreeClassifier()\n",
      "                model.fit(X_train,y_train)\n",
      "                y_pred_DT = model.predict(X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT))\n",
      "                print(accuracy_score(y_test, y_pred_DT))\n",
      "\n",
      "            DT After applying PCA:\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                from sklearn.decomposition import PCA\n",
      "                pca = PCA()\n",
      "                X_train_2 = pca.fit_transform(X_train)\n",
      "                X_test_2 = pca.transform(X_test)\n",
      "                explained_variance = pca.explained_variance_ratio_\n",
      "                from sklearn import tree\n",
      "                model2=tree.DecisionTreeClassifier(random_state=1)\n",
      "                model2.fit(X_train_2,y_train)\n",
      "                y_pred_DT_2 = model2.predict(X_test_2)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT_2), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT_2))\n",
      "                print(accuracy_score(y_test, y_pred_DT_2))\n",
      "\n",
      "            DT After Applying LDA:\n",
      "                from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "                model = LinearDiscriminantAnalysis()\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                sc = StandardScaler()\n",
      "                X_train = sc.fit_transform(X_train)\n",
      "                X_test = sc.transform(X_test)\n",
      "                model.fit(X_train,y_train)\n",
      "                y_pred = model.predict(X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n",
      "                print(classification_report(y_test,y_pred))\n",
      "                print(accuracy_score(y_test, y_pred))\n",
      "\n",
      "                LDA Hyperparameter Tuning:\n",
      "                    from sklearn.model_selection import GridSearchCV\n",
      "                    from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "                    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "                    model = LinearDiscriminantAnalysis()\n",
      "                    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "                    grid = dict()\n",
      "                    grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
      "                    search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "                    results = search.fit(X_train, y_train)\n",
      "                    print('Mean Accuracy: %.3f' % results.best_score_)\n",
      "                    print('Config: %s' % results.best_params_)\n",
      "\n",
      "                    (With the Best Params, repeat the above LDA steps with Best Params)\n",
      "\n",
      "            DT After Applying Kernel PCA: ( Too Complex, Will not be asked! )\n",
      "\n",
      "            DT After Applying MCA:\n",
      "                ! pip install prince\n",
      "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "                cat = []\n",
      "                num = []\n",
      "                for i in X.columns:\n",
      "                    if X[i].nunique() < X.shape[0]/4:\n",
      "                        cat.append(i)\n",
      "                    else:\n",
      "                        num.append(i)\n",
      "                X_train_cat = X_train[cat]\n",
      "                X_train_num = X_train[num]\n",
      "                X_test_cat = X_test[cat]\n",
      "                X_test_num = X_test[num]\n",
      "                import prince\n",
      "                from sklearn.metrics import make_scorer\n",
      "                from sklearn.metrics import explained_variance_score\n",
      "                mca = prince.MCA()\n",
      "                mca_X_train = mca.fit_transform(X_train_cat)\n",
      "                mca_X_test = mca.fit_transform(X_test_cat)\n",
      "                main_mca_X_train = pd.concat([mca_X_train,X_train_num],axis = 1)\n",
      "                main_mca_X_test = pd.concat([mca_X_test,X_test_num],axis = 1)\n",
      "                explained_var = make_scorer(explained_variance_score)\n",
      "                explained_var\n",
      "                from sklearn import tree\n",
      "                model2=tree.DecisionTreeClassifier()\n",
      "                model2.fit(main_mca_X_train,y_train)\n",
      "                y_pred_DT_2 = model2.predict(main_mca_X_test)\n",
      "                sns.heatmap(confusion_matrix(y_test, y_pred_DT_2), annot=True)\n",
      "                print(classification_report(y_test,y_pred_DT_2))\n",
      "                print(accuracy_score(y_test, y_pred_DT_2))\n",
      "\n",
      "\n",
      "\n",
      "        Recommendation Systems:\n",
      "\n",
      "            Popularity Based Recommendation:\n",
      "                x=df.groupby('ItemID').agg({'Rating':'mean','ItemID':'count'})\n",
      "                x.rename(columns={'ItemID':'CountofReviews'},inplace=True)\n",
      "                x[x['CountofReviews']>50].sort_values(by='Rating',ascending=False).head(10)\n",
      "\n",
      "            Content Based Recommendation:\n",
      "                Preprocessing:\n",
      "                    genres=data1['genres'].str.split('|',expand=True)\n",
      "                    genres=genres.fillna('Others')\n",
      "                    genres.columns=['genre1','genre2','genre3']\n",
      "                    data1=pd.concat([data1,genres],axis=1)\n",
      "                    data2=data1[movie_feat]\n",
      "                    data3=pd.get_dummies(data2)\n",
      "                    data3=data3.dropna()\n",
      "                \n",
      "                from sklearn.neighbors import NearestNeighbors\n",
      "                rec_model = NearestNeighbors(metric = 'cosine')\n",
      "                rec_model.fit(data3)\n",
      "                query_movie_index=200\n",
      "                dist, ind = rec_model.kneighbors(data3.iloc[query_movie_index, :].values.reshape(1, -1), n_neighbors = 6)\n",
      "                list(data3.index[ind[0]])[1:]\n",
      "                for i in range(0, len(dist[0])):\n",
      "                    if i == 0:\n",
      "                        print('Top 5 Recommendations for the user who watched the movie :',data3.index[query_movie_index])\n",
      "                    else:\n",
      "                        print(i, data3.index[ind[0][i]])\n",
      "\n",
      "\n",
      "            Collaborative Recommendation:\n",
      "                from surprise import Dataset,Reader\n",
      "                from surprise.model_selection import train_test_split,cross_validate\n",
      "                from surprise import KNNWithMeans,SVDpp\n",
      "                from surprise import accuracy\n",
      "                reader=Reader(rating_scale=(1,5))\n",
      "                rating_data=Dataset.load_from_df(data_recom[['UserID','ItemID','Rating']],reader)\n",
      "                [trainset,testset]=train_test_split(rating_data,test_size=0.15,shuffle=True)\n",
      "                trainsetfull=rating_data.build_full_trainset()\n",
      "                print(\"Number of users :\",trainsetfull.n_users,'\n",
      "')\n",
      "                print(\"Number of items :\",trainsetfull.n_items,'\n",
      "')\n",
      "                my_k = 15\n",
      "                my_min_k = 5\n",
      "                my_sim_option = {'name':'pearson', 'user_based':False}\n",
      "                algo = KNNWithMeans(k = my_k, min_k = my_min_k, sim_options = my_sim_option, verbose = True)\n",
      "                results = cross_validate(\n",
      "                    algo = algo, data = rating_data, measures=['RMSE'], \n",
      "                    cv=5, return_train_measures=True\n",
      "                    )\n",
      "                print(results['test_rmse'].mean())\n",
      "                algo.fit(trainsetfull)\n",
      "                algo.predict(uid = 50, iid =2)\n",
      "                len(ratings['movieId'].unique())\n",
      "                item_id_unique=ratings['movieId'].unique()\n",
      "                item_id10=ratings.loc[ratings['userId']==10,'movieId']\n",
      "                item_id_pred=np.setdiff1d(item_id_unique,item_id10)\n",
      "                testset=[[50,iid,5] for iid in item_id_pred]\n",
      "                pred=algo.test(testset)\n",
      "                pred\n",
      "                pred_ratings=np.array([pred1.est for pred1 in pred])\n",
      "                i_max=pred_ratings.argmax()\n",
      "                iid=item_id_pred[i_max]\n",
      "                print(\"Top item for user 10 has iid {0} with predicted rating {1}\".format(iid,pred_ratings[i_max]))\n",
      "\n",
      "                Compare Different Algorithms Scoreboard:\n",
      "\n",
      "                    from surprise import NormalPredictor\n",
      "                    from surprise import KNNBasic\n",
      "                    from surprise import KNNWithMeans\n",
      "                    from surprise import KNNWithZScore\n",
      "                    from surprise import KNNBaseline\n",
      "                    from surprise import SVD\n",
      "                    from surprise import BaselineOnly\n",
      "                    from surprise import SVDpp\n",
      "                    from surprise import NMF\n",
      "                    from surprise import SlopeOne\n",
      "                    from surprise import CoClustering\n",
      "                    benchmark = []\n",
      "                    # Iterate over all algorithms\n",
      "                    algorithms = [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]\n",
      "                    print (\"Attempting: \", str(algorithms), '\n",
      "\n",
      "\n",
      "')\n",
      "                    for algorithm in algorithms:\n",
      "                        print(\"Starting: \" ,str(algorithm))\n",
      "                        results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
      "                        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
      "                        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
      "                        benchmark.append(tmp)\n",
      "                        print(\"Done: \" ,str(algorithm), \"\n",
      "\n",
      "\")\n",
      "                    print ('\n",
      "\tDONE\n",
      "')\n",
      "                    surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
      "                    surprise_results\n",
      "\n",
      "            Apriori Algorithm:\n",
      "                from mlxtend.frequent_patterns import apriori\n",
      "                from mlxtend.frequent_patterns import association_rules\n",
      "\n",
      "                unique_row_items = []\n",
      "                for index, row in df.iterrows():\n",
      "                    items_series = str(row[0]).split(',')\n",
      "                    for item in items_series:\n",
      "                        if item not in unique_row_items:\n",
      "                            unique_row_items.append(item)\n",
      "                df_apriori = pd.DataFrame(columns=unique_row_items)\n",
      "                for index, row in df.iterrows():\n",
      "                    items = str(row[0]).split(',')\n",
      "                    one_hot_encoding = np.zeros(len(unique_row_items),dtype=int)\n",
      "                    for it in items:\n",
      "                        for i,column in enumerate(df_apriori.columns):\n",
      "                            #print(i,column,it)\n",
      "                            if it == column:\n",
      "                                one_hot_encoding[i] = 1\n",
      "                    df_apriori.at[index] = one_hot_encoding\n",
      "                df_apriori=df_apriori.astype('int')\n",
      "                freq_items = apriori(df_apriori, min_support = 0.2, use_colnames = True, verbose = 1)\n",
      "                df_association_rules = association_rules(freq_items, metric = \"confidence\", min_threshold = 0.2)\n",
      "                df_association_rules.sort_values(\"confidence\",ascending=False)\n",
      "                cols = ['antecedents','consequents']\n",
      "                df_association_rules[cols] = df_association_rules[cols].applymap(lambda x: tuple(x))#.apply(lambda x: str(x))\n",
      "                print (df_association_rules)\n",
      "                df_association_rules = (df_association_rules.explode('antecedents')\n",
      "                    .reset_index(drop=True)\n",
      "                    .explode('consequents')\n",
      "                    .reset_index(drop=True))\n",
      "                df_association_rules[\"product_group\"] = df_association_rules[\"antecedents\"].apply(lambda x: str(x)) + \",\" + df_association_rules[\"consequents\"].apply(lambda x: str(x))\n",
      "                df1 = df_association_rules.loc[:,[\"product_group\",\"confidence\",\"lift\"]].sort_values(\"confidence\",ascending=False)\n",
      "            \n",
      "                Plot:\n",
      "                    sns.barplot(x=\"product_group\",y=\"confidence\",data=df1)\n",
      "                    sns.barplot(x=\"product_group\",y=\"confidence\",hue=\"lift\",data=df1);\n",
      "                    df1.plot.bar()\n",
      "\n",
      "                Example Conclusion:\n",
      "                    * 80% of customers who buy MAGGI (Instant soup) buy it in tea.\n",
      "                    * TEA and MAGGI products increase their sales by 2.17 times mutually.\n",
      "                    * 66% of customers who buy SUGAR buy it in bread.\n",
      "                    * 42% of customers who buy COFFEE buy sugar and CORNFLAKES. At the same time, 33% of these sales are in bread.\n",
      "                \n",
      "            Hybrid Recommendation System: ( Too Complicated, wont be asked. )\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(ML3.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f808da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<weak style=\"font-size:12px\">\n",
       "        Theory:\n",
       "        \n",
       "            Supervised vs Unsupervised Learning:\n",
       "                ● Supervised Learning:\n",
       "                    ● Finding model that mapsthe target variable to input variables\n",
       "                    ● Consists of inputs and labels\n",
       "                ● Unsupervised Learning:\n",
       "                    ● Aims to learn more about the given data\n",
       "                    ● Has no Labels to map\n",
       "                \n",
       "            Clustering:\n",
       "                \n",
       "                Types of Clustering:\n",
       "                    ● Density Based: Clusters formed based on density of nodes\n",
       "                    ● Hierarchical Based: Formed based on distances between nodes\n",
       "                    ● Graph Based: Dividing a set of graphs or dividing the nodes of graph\n",
       "                    ● Partition Based: Partitioned into pre-determined number of clusters\n",
       "                    ● Model Based: Assumes that data is a mixture of distributions and tries to fit a model such that each distrubution represents a cluster.\n",
       "                    \n",
       "                Proximity/Similarity Measures:\n",
       "                    ● Manhattan (L1)\n",
       "                    ● Euclidean (L2)\n",
       "                    ● Minkowski (LN)\n",
       "                    ● Chebychev (L-inf)\n",
       "                \n",
       "            K-Means Algorithm:\n",
       "            \n",
       "                Purpose:\n",
       "                    ● Used when data is numeric\n",
       "                    ● Recursive Technique\n",
       "                    ● Cannot train a model\n",
       "                    ● Based on Proximity measures.\n",
       "                    ● Greedy Algorithm\n",
       "                    ● Minimizes sq. error of points in cluster\n",
       "                    ● Non-Deterministic Algorithm.\n",
       "                \n",
       "                K-Means Procedure:\n",
       "                    ● Choose a Distnace measure and K\n",
       "                    ● Randomly choose K points as cluster centroids\n",
       "                    ● Assign nearest cluster centroid to each point\n",
       "                    ● Compute means of clusters\n",
       "                    ● Reassign cluster based on means\n",
       "                    ● Repeat above 2 steps until cluster means dont change\n",
       "                    \n",
       "                K-Means Advantages:\n",
       "                    ● Easy to Understand\n",
       "                    ● Simple Implementation\n",
       "                    \n",
       "                K-Means Disadvantages:\n",
       "                    ● Finding Optimal K is intensive to compute\n",
       "                    ● Initial Centroid assignment affects output (Non-deterministic)\n",
       "                    ● Ineffeicient for outliers\n",
       "                \n",
       "                For categorical, use K-Modes Algorithm.\n",
       "                For mixed, use K-Prototypes Algorithm.\n",
       "                \n",
       "                Finding Optimal K:\n",
       "                    ● Elbow Method:\n",
       "                        ● Tries to reduce WCSS\n",
       "                    ● Silhouette Score Steps:\n",
       "                        ● For each point, calculate Silhouette Coefficient\n",
       "                        ● Calculate mean intra-cluster distance for that point\n",
       "                        ● Calculate mean inter-cluster distance for that point\n",
       "                        ● Calculate Si using obtained ai and bi\n",
       "                        ● Similarly, calculate silhouette coefficient for each observation\n",
       "                        ● Take average of all, this is the silhouette score.\n",
       "                        \n",
       "                Hierarchical/Agglometarive Clustering:\n",
       "                    \n",
       "                    Agglomerative Clustering steps:\n",
       "                        Consider each point as a unique cluster\n",
       "                        Calculate pairwise distance between all clusters.\n",
       "                        Combine 2 nearest clusters into a single cluster.\n",
       "                        Calculate distance between newly formed cluster and remaining clusters\n",
       "                        Repeat above 2 steps until a single cluster is formed.\n",
       "                        \n",
       "                    Linkage Methods:\n",
       "                        \n",
       "                        Single Linkage:\n",
       "                            ● It is defined as the minimum distance between the points of two clusters.\n",
       "                            ● The method can create the non-elliptical clusters \n",
       "                            ● It can produce undesirable results in the presence of outliers\n",
       "                            ● It causes a chaining effect, where clusters have merged since at least one point in a cluster is closest to a point in another cluster. \n",
       "                            ● This forms a long and elongated cluster.\n",
       "                            \n",
       "                            Adv: \n",
       "                                ● Can create non-elliptical clusters\n",
       "                            Disadv:\n",
       "                                ● Sensitive to outliers\n",
       "                                ● Prone to chaining effect\n",
       "\n",
       "                        Complete Linkage:\n",
       "                            ● It is defined as the maximum distance between the points of the two different clusters.\n",
       "                            ● The method returns more stable clusters, with nearly equal diameter\n",
       "                            ● It avoids the chaining effect\n",
       "                            ● It is less sensitive to outliers\n",
       "                            ● It breaks the large clusters and it is biased towards globular clusters\n",
       "                            \n",
       "                            Adv:\n",
       "                                ● Creates more compact clusters\n",
       "                            Disadv:\n",
       "                                ● Biased towards globular clusters\n",
       "                                ● Breaks large clusters\n",
       "                                \n",
       "                        Average Linkage:\n",
       "                            ● It is defined as the average of all the pairwise distances between the two clusters.\n",
       "                            ● This method balances between the single and complete linkage\n",
       "                            ● It forms compact clusters and the method is robust to outliers\n",
       "                            \n",
       "                            Adv:\n",
       "                                ● Balances between single and complete linkage\n",
       "                                ● Robust to outliers\n",
       "                            Disadv:\n",
       "                                ● Biased towards globular clusters\n",
       "                                \n",
       "                        Centroid Linkage:\n",
       "                            ● It is defined as the distance between the centroids (means) of the two clusters.\n",
       "                            ● It creates similar clusters as average linkage\n",
       "                            ● Problem: A smaller cluster can be more similar to the newly merged larger cluster rather than the individual clusters (inversion).\n",
       "                            \n",
       "                            Adv:\n",
       "                                ● Cluster formation is similar to average linkage\n",
       "                            Disadv:\n",
       "                                ● Can cause inversion\n",
       "\n",
       "                        Ward Linkage (ward minimum variance method):\n",
       "                            ● The clusters are merged; if the new cluster minimizes the variance \n",
       "                            ● It is a computationally intensive method\n",
       "                            \n",
       "                            Adv:\n",
       "                                ● Most effective in presence of outliers\n",
       "                            Disadv:\n",
       "                                ● Biased towards globular clusters\n",
       "\n",
       "                    Dendrogram:\n",
       "                        \n",
       "                        Advantages:\n",
       "                            ● Does not require a pre-specified number of clusters\n",
       "                            ● Hierarchical relation between the clusters can be identified\n",
       "                            ● Dendrogram provides a clear representation of clusters\n",
       "                        Disadvantages:\n",
       "                            ● Different dendrograms are produced for different linkage methods\n",
       "                            ● Selecting an optimal number of clusters using dendrogram is sometimes difficult\n",
       "                            ● Time complexity is high\n",
       "                            \n",
       "                Density Based Clustering (DBSCAN) (Density-Based Spatial Clustering of Applications with Noise):\n",
       "                    \n",
       "                    DBSCAN Procedure:\n",
       "                        ● Decide Epsilon and Min_Samples\n",
       "                        ● Choose a starting point (P) randomly and find Epsilon neighbourhood\n",
       "                        ● If P is a core point, find all density-reachable points from P and forma cluster, else mark P as Noise.\n",
       "                        ● Find next unvisited point and repeat.\n",
       "                        ● Repeat this process till all points are visited.\n",
       "                        \n",
       "                    Advantages:\n",
       "                        ● Does not require a pre-specified number of clusters\n",
       "                        ● Useful to form clusters of any size and shape\n",
       "                        ● Can be used to find outliers in the data\n",
       "                    Disadvantages:\n",
       "                        ● Can not efficiently work with the clusters of varying densities\n",
       "                        ● Does not work well with high dimensional data\n",
       "\n",
       "                Dimension Reduction Techniques:\n",
       "                    \n",
       "                    Note of Curse of Dimensionality:\n",
       "                        ● If the number of features 'n' is large, the number of samples m, may be too small for accurate parameter estimation\n",
       "                        ● covariance matrix has n2 parameters\n",
       "                        ● For accurate estimation, sample size should be much bigger than n2,to be able to accurately represent the covariance, otherwise the model may become too complicated for the data, overfitting\n",
       "                        ● If m < n2 we assume that features are uncorrelated (because we cannot represent it accurately with the \n",
       "                        given m points), even if we know this assumption is wrong. Doing so, we do not represent the covariance \n",
       "                        as parameters in our model.\n",
       "                        \n",
       "                    Dimension Reduction Techniques:\n",
       "                        \n",
       "                        PCA Procedure:\n",
       "                            ● Standardize Data\n",
       "                            ● Compute CoV matrix\n",
       "                            ● Calculate Eigenvalues and Eignevectors\n",
       "                            ● Sort Eigenvalues in Desc Order\n",
       "                            ● Select Eigenvectors that explain max variance in data.\n",
       "                        PCA Applications:\n",
       "                            ● PCA is mainly used in image compression, facial recognition models\n",
       "                            ● It is also used in the exploratory analysis to reduce the dimension of data before applying machine learning methods\n",
       "                            ● Used in the field of psychology, finance to identify the patterns high dimensional data\n",
       "                        \n",
       "                        A Note on PCA Terminologies:\n",
       "                            ● Covariance:\n",
       "                                ● The covariance measures how co-dependent two variables are\n",
       "                                ● Positive covariance value means that the two variables are directly proportional to each other\n",
       "                                ● Negative covariance value means that the two variables are inversely proportional to each other\n",
       "                                ● It is similar to variance, but the variance illustrates the variation of the single variable and covariance explains how two variables vary together\n",
       "                            ● Covariance Matrix:\n",
       "                                ● The covariance matrix explains the covariance between the pair of variables \n",
       "                                ● The diagonal entries represent the variance of the variable, as it is the covariance of the variable with itself\n",
       "                                ● The diagonal matrix is always symmetric\n",
       "                                ● The off-diagonal entries are covariance between the variables that represent the distortions (redundancy) in the data\n",
       "                            ● Eigenvalue:\n",
       "                                ● For any nxn matrix A, we can find n eigenvalues that satisfy the characteristic equation\n",
       "                                ● A characteristic equation is defined as: |A - λI| = 0 i.e. det(A - λI) = 0\n",
       "                                ● The characteristic polynomial for matrix A given as |A - λI|\n",
       "                                ● The scalar value λ is known as the eigenvalue of the matrix A\n",
       "                                ● Eigenvalues can be real/ complex in nature\n",
       "                            ● Eigenvectors:\n",
       "                                ● For each eigenvalue λ of a matrix A, there exist a non-zero vector x, which satisfy the equation: (A - λI)x = 0 i.e. Ax = λx\n",
       "                                ● The vector x is known as the eigenvector corresponding to the eigenvalue λ\n",
       "                                ● Eigenvectors are always orthogonal to each other\n",
       "                                ● The eigenvector is a vector that does not changes its direction, after transformation by matrix A\n",
       "                                \n",
       "                        LDA Goal:\n",
       "                            ● Maximize the distance between the means (i.e. between μ1 and μ2) of classes\n",
       "                            ● Minimize the variance within each class\n",
       "                            \n",
       "                        LDA Procedure:\n",
       "                            ● Standardize Data\n",
       "                            ● Compute the within class matrix\n",
       "                            ● Compute the between class matrix\n",
       "                            ● Find projection vectors and transform the data.\n",
       "                            \n",
       "                        A Note on LDA Terminologies:\n",
       "                            ● Within Class Matrix (Sw):\n",
       "                                ● It captures how precisely the data is scattered within the class\n",
       "                                ● Consider the data divided into two classes C1 and C2, the within class matrix is given by \n",
       "                                the summation of the covariance matrix (S1) of the class C1 and the covariance matrix (S2) of the class C2.\n",
       "                                    Sw = S1 + S2\n",
       "                            ● Between Class Matrix (Sb):\n",
       "                                It represents how precisely the data is scattered across the classes\n",
       "                                Suppose the means of classes C1 and C2 are μ1 and μ2 respectively. The formula to find SB is given as:\n",
       "                                    Sb = (μ1 - μ2).(μ1 - μ2)T\n",
       "\n",
       "                        PCA vs LDA:\n",
       "                            PCA:\n",
       "                                ● It is described as an unsupervised machine learning technique\n",
       "                                ● It considers the independent features to find principal components that maximize the variance in the data\n",
       "                            LDA:\n",
       "                                ● It is described as a supervised machine learning technique\n",
       "                                ● It calculates the linear discriminants to obtain the maximum separation between the classes of the dependent variable\n",
       "                                \n",
       "                        Kernel PCA:\n",
       "                            ● Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable. \n",
       "                            ● It is similar to the idea of Support Vector Machines.\n",
       "                            ● There are various kernel methods like linear, polynomial, and gaussian.\n",
       "\n",
       "                        Limitations in Kernel PCA:\n",
       "                            ● Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated \n",
       "                            variables into a set of values of linearly uncorrelated variables called principal \n",
       "                            components (from Wikipedia). \n",
       "                            ● PCA assumes that the data contains continuous values only and contains no categorical variables.\n",
       "                            ● It is not possible to apply PCA techniques for dimensionality reduction when the data is composed of categorical variables.\n",
       "\n",
       "                Recommendation Systems:\n",
       "                    \n",
       "                    Types:\n",
       "                        ● Popularity Based\n",
       "                        ● Content Based\n",
       "                        ● Collaborative Filtering\n",
       "                            ● User Based\n",
       "                            ● Item Based\n",
       "                        ● Matrix Factorization\n",
       "                        ● Association Rule\n",
       "                        ● Hybrid\n",
       "                        \n",
       "                    Popularity Based System :\n",
       "                        \n",
       "                        Procedure:\n",
       "                            ● Read the dataset\n",
       "                            ● Use group by or SQL merge concept to get the ratings for each item/movie\n",
       "                            ● Take mean of all the item/movie\n",
       "                            ● Sort it in descending order\n",
       "                            ● Recommend top ten or five to a new user\n",
       "                            \n",
       "                            ● Takes only mean into consideration\n",
       "                            ● User bias can be removed by deducting mean rating of a user for each item.\n",
       "\n",
       "                    Content Based System:\n",
       "                        \n",
       "                        Procedure:\n",
       "                            ● Read data\n",
       "                            ● Preprocessing\n",
       "                            ● Distance matrix calculation\n",
       "                            ● Check similarity\n",
       "                            ● Recommend\n",
       "\n",
       "                        Advantages:\n",
       "                            ● Do not require a lot of user's data\n",
       "                            ● Only item data is sufficient\n",
       "                            ● Does not suffer from cold start problem\n",
       "                            ● Less expensive to build and maintain\n",
       "                        Disadvantages:\n",
       "                            ● Feature availability\n",
       "                            ● Less dynamic\n",
       "                            ● Diversity is missing\n",
       "                            \n",
       "                    Types of Similarities:\n",
       "                        ● Euclidean Similarity\n",
       "                        ● Cosing Similarity\n",
       "                        ● Pearson Similarity\n",
       "                        ● Jaccard Similarity\n",
       "                        \n",
       "                    Collaborative Filtering:\n",
       "                        \n",
       "                        Types:\n",
       "                            Memory Based:\n",
       "                                ● Similarity between users and/or items are calculated and used as weights to predict a rating\n",
       "                                ● Approach collaborative filtering problem using the entire dataset\n",
       "                                ● Not learning any parameter here. Non-parametric approaches.\n",
       "                                ● Quality of prediction is good\n",
       "                                ● Scalability is an issue\n",
       "                                Eg - Item based, User based, kNN clustering\n",
       "                            Model Based:\n",
       "                                ● Uses statistics and machine learning methods on rating matrix\n",
       "                                ● Speed and scalability issues can be addressed\n",
       "                                ● Can solve problem of huge database & sparse matrix\n",
       "                                ● Better at dealing with sparsity\n",
       "                                ● Predicts ratings of unrated items\n",
       "                                ● Inference is not traceable due to hidden factors\n",
       "                                Eg-Matrix factorization ( SVD, PMF, NMF), Neural nets based\n",
       "                        \n",
       "                        Types: (In Memory Based):\n",
       "                            ● User Based:\n",
       "                                ● Similar users are considered\n",
       "                                ● “Users who are similar to you also liked…”\n",
       "                            ● Item Based:\n",
       "                                ● Similar items are considered\n",
       "                                ● “Users who liked this item also liked…”\n",
       "                            \n",
       "                        Challenges with Collaborative Filtering:\n",
       "                            ● Cold Start problem: The model requires enough amount of other users already in the system to find a good match.\n",
       "                            ● Sparsity: If the user/ratings matrix is sparse, and it is not easy to find the users that have rated the same items.\n",
       "                            ● Popularity Bias: Not possible to recommend items to users with unique tastes.\n",
       "                                ● Popular items are recommended more to the users as more data being available on them\n",
       "                                ● This may begin a positive feedback loop not allowing the model to recommend items with less popularity to the users\n",
       "                            ● Shilling attacks: Users can create fake preferences to push their own items\n",
       "                            ● Scalability: Collaborative filtering models are computationally expensive\n",
       "\n",
       "                        Steps in Collaborative Filtering:\n",
       "                            ● Determine similar users\n",
       "                                ● Calculate similarity matrix using similarity distance and user-item ratings. Get top similar neighbors\n",
       "                            ● Estimate rating that a user would give to an item based on the ratings of similar users\n",
       "                                ● Estimated rating R for a user U for an item I would be close to average rating given to I by the top n users most similar to U\n",
       "                                ● Ru = (∑nu=1Ru)/n\n",
       "                                ● Weighted average - multiply each rating by similarity factor\n",
       "                            ● Accuracy of estimated ratings \n",
       "                                ● RMSE ( root mean squared error) \n",
       "                                ● MAE ( mean absolute error)\n",
       "\n",
       "                    Market Basket Analysis:\n",
       "                        ● Uncovers association between items.\n",
       "                        ● Identifies pattern of co-occurrence\n",
       "                        ● Market basket analysis may provide the retailer with information to understand the behaviour of a buyer.\n",
       "                        “Customers who bought book A also bought book B”\n",
       "                        \n",
       "                        Terminologies in Merket Basket:\n",
       "                            ● Itemset - a collection of items purchased by a customer\n",
       "                                ● Ex - {Pizza, pepsi, garlic bread}\n",
       "                            ● Support count (o )- Frequency of occurrence of an itemset.\n",
       "                                ● Ex- o( Pizza, pepsi, garlic bread) = 2\n",
       "                            ● Support - fraction of transaction that contains itemset\n",
       "                                ● Ex- S( Pizza, pepsi, garlic bread) = ⅖\n",
       "                            ● Frequent Itemset - An itemset whose support isgreater than or equal to a min_sup threshold\n",
       "\n",
       "                    Apriori Algorithm Steps:\n",
       "                        ● Set a min. support and confidence\n",
       "                        ● Take all the subsets in transactions having higher support than min support\n",
       "                        ● Take all the rules of these subsets having higher confidence than min confidence\n",
       "                        ● Sort the rules by decreasing lift\n",
       "                        \n",
       "                    Hybrid Algorithm:\n",
       "                        ● Combination of multiple algorithm\n",
       "                        ● Customized algorithm\n",
       "                        \n",
       "                        Methods of Hybrid Algorithm:\n",
       "                            ● Weighted - Each system is weighted to calculate final recommendation\n",
       "                            ● Switching - System switches between different recommendation model\n",
       "                            ● Mixed - Recommendations from different models are presented together.\n",
       "                            \n",
       "                    Evaluation Metrics of Recommendation Systems:\n",
       "                        ● User Satisfaction\n",
       "                            ○ Subjective metric\n",
       "                            ○ Measured by user survey or online experiments\n",
       "                        ● Prediction Accuracy\n",
       "                            ○ Rating Prediction (MAE, RMSE)\n",
       "                            ○ Top-N Recommendation (Precision, Recall)\n",
       "                        ● Coverage\n",
       "                            ○ Ability to recommend long tail items ( entropy, gini index)\n",
       "                        ● Diversity\n",
       "                            ○ Ability to cover user's different interests\n",
       "                        ● Novelty - Ability of Recommendation system to recommend long tail items and new items.\n",
       "                        ● Trust - Trust increases the interaction of user to recommendation system.\n",
       "                            ○ Transparency, social\n",
       "                        ● Robust - Ability of Recommendation system to prevent attacks.\n",
       "                            ○ Shilling attack\n",
       "                        ● Real Time - Generate new recommendation when user has new behaviours immediately.\n",
       "        </weak>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown('<weak style=\"font-size:12px\">{}</weak>'.format(ML3.st_theory.__doc__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "algorithms = [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]\n",
    "print (\"Attempting: \", str(algorithms), '\n",
    "\n",
    "')\n",
    "for algorithm in algorithms:\n",
    "    print(\"Starting: \" ,str(algorithm))\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    print(\"Done: \" ,str(algorithm), \"\n",
    "\n",
    "\")\n",
    "print ('\n",
    "DONE\n",
    "')\n",
    "surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
    "surprise_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f7142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed933a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897de44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a3990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa89ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9584bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b3a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f0a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f56af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37757e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91b09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e406bec50f52913348369e3dba25a643ebdf189eb91a17200e2168ecbed8ac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
